# -*- coding: utf-8 -*-
"""Geoffrey Korir-Week4 Assignment Model Quality and Improvements Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4shuxVekYOWRsFoP_7TwSOaWwjMdIJD

#Model Quality and Improvements

##Problem Statement

As a data professional working for a pharmaceutical company, you need to develop a model that predicts whether a patient will be diagnosed with diabetes. The model needs to have an accuracy score greater than 0.85.

##Data Importation
"""

# Import libraries 
import pandas as pd

# Dataset URL: https://bit.ly/DiabetesDS

diabetes_Ds_df = pd.read_csv("https://bit.ly/DiabetesDS")

diabetes_Ds_df.head()

"""##Data Exploration"""

# Check the dataset size
diabetes_Ds_df.shape

# Check for NaN values 
diabetes_Ds_df.isna().sum()

# Describe the Data
diabetes_Ds_df.info()

# Summary stats for the data set
diabetes_Ds_df.describe()

#see the amount of data on each target
diabetes_Ds_df['Outcome'].value_counts()

# Expressing the outcome as a percentage
print('1. The percentage of patients without diabetes are ' 
      + str(round(((diabetes_Ds_df["Outcome"].isin([0]).sum())/diabetes_Ds_df.shape[0])*100,2)) + ' %')
print('2. The percentage of patients with diabetes are ' 
      + str(round((diabetes_Ds_df["Outcome"].isin([1]).sum())/diabetes_Ds_df.shape[0])*100,2)) + ' %')

"""##Data Cleaning"""

# check column names
diabetes_Ds_df.columns

# Standardize a dataset by stripping leading and trailing spaces and setting all columns to lower
diabetes_Ds_df.columns = diabetes_Ds_df.columns.str.strip().str.lower()
diabetes_Ds_df.columns

# Check for missing values
diabetes_Ds_df.isna().sum()

"""There are no missing values in the data set"""

# Check for duplicates
diabetes_Ds_df.duplicated().sum()

"""There are no duplicates in the data

##Data Preparation
"""

# check correlation of features and target
import matplotlib.pyplot as plt
import seaborn as sns

features = diabetes_Ds_df.columns
corr_= diabetes_Ds_df[features].corr()
plt.figure(figsize=(11,7))
sns.heatmap(corr_, annot=True, fmt = ".2f", cmap = "coolwarm");

# plots of relationship between features and target 
for feature in features[:-1]:
  plt.hist(diabetes_Ds_df[diabetes_Ds_df['outcome']==1][feature], color= 'blue', alpha = 0.7, label = 'Diabetic', density=True)
  plt.hist(diabetes_Ds_df[diabetes_Ds_df['outcome']==0][feature], color= 'red', alpha = 0.7, label = 'Not Diabetic', density=True)
  plt.title(feature)
  plt.ylabel('Probability')
  plt.xlabel(feature)
  plt.legend()
  plt.show()

"""**Conclusion**

Features and targets have the same relationship. when features and targets increases probability behaves the same for features and targets

##Data Modeling
"""

diabetes_Ds_df.head()

#(Using Decision Trees, Random Forest and Logistic Regression)
#we define features and target

#get functions from sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split

features = diabetes_Ds_df.drop(['outcome'], axis=1)
target = diabetes_Ds_df['outcome']

x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=142)

# train models
desc_model = DecisionTreeClassifier(random_state=12345)
forest_model = RandomForestClassifier(random_state=12345)
regression_model = LogisticRegression(random_state=12345, solver='liblinear')


desc_model.fit(x_train, y_train)
forest_model.fit(x_train, y_train)
regression_model.fit(x_train, y_train)

# make predictions
desc_test_predictions =desc_model.predict(x_test)
forest_test_predictions =forest_model.predict(x_test)
regression_test_predictions =regression_model.predict(x_test)

"""##Model Evaluation"""

# calculate accuracy score
# Checking accuracy of our model
from sklearn.metrics import accuracy_score 
print(f'DecisionTreeClassifier accuracy: {accuracy_score(y_test, desc_test_predictions)}')
print(f'RandomForestClassifier accuracy: {accuracy_score(y_test, forest_test_predictions)}')
print(f'LogisticRegression accuracy: {accuracy_score(y_test, regression_test_predictions)}')

from sklearn.metrics import classification_report
# print classification report for DecisionTreeClassifier
print(f'DecisionTreeClassifier classification report:\n {classification_report(y_test, desc_test_predictions)}')

print(f'RandomForestClassifier classification report:\n {classification_report(y_test, forest_test_predictions)}')

print(f'LogisticRegression classification report:\n {classification_report(y_test, regression_test_predictions)}')

"""##Hyparameter Tuning"""

# Tuning hyperparameters for Decision tree classifier 
for depth in range(1, 20):
    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)
    model.fit(x_train, y_train)
    predictions_valid = model.predict(x_test)
    print('max_depth =', depth, ': ', end='')
    print(accuracy_score(y_test, predictions_valid))

# best maximum depth is 2
model = DecisionTreeClassifier(random_state=12345, max_depth=2)
model.fit(x_train, y_train)
predictions_valid = model.predict(x_test)
print(accuracy_score(y_test, predictions_valid))

# Tune hyperparameters for RandomForestClassifier()

def get_estimators():
  score = 0
  for estimators in range(1, 51):
      model = RandomForestClassifier(random_state=12345, n_estimators=estimators)
      model.fit(x_train, y_train)
      predictions_valid = model.predict(x_test)
      pred_score = accuracy_score(y_test, predictions_valid)
      if pred_score > score: 
          score = pred_score
  return print('n_estimators =', estimators, 'accuracy: ', score)

get_estimators()

# Tune hyperparameters for LogisticRegression
# You don't need to tune the hyperparameters of logistic regression. Just train it.
model = LogisticRegression(random_state=12345, solver='liblinear')
model.fit(x_train, y_train)
predictions_valid = model.predict(x_test)
print(f'accuracy score is: {accuracy_score(y_test, predictions_valid)}')

"""Conclusion and recomendation

Logistics regression model perfoms better than DecisionTreeClassifier and RandomForestClassifier
"""